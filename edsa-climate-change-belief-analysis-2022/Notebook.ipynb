{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd943e8",
   "metadata": {},
   "source": [
    "# Advanced Classification Predict\n",
    "\n",
    "©  Explore Data Science Academy\n",
    "\n",
    "---\n",
    "\n",
    "### Honour Code\n",
    "\n",
    "I **Team_JM1**, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "<img src=\"climate_change.jpg\" width=\"800px\">\n",
    "    <figcaption><p text_align = \"center\">\n",
    "\n",
    "### Climate Change\n",
    "Climate is the average weather in a place over many years. Climate change is a shift in those average conditions.\n",
    "\n",
    "The rapid climate change we are now seeing is caused by humans using oil, gas and coal for their homes, factories and transport.\n",
    "\n",
    "When these fossil fuels burn, they release greenhouse gases - mostly carbon dioxide (CO2). These gases trap the Sun's heat and cause the planet's temperature to rise.\n",
    "\n",
    "### Predict Overview: Climate Change Belief Analysis 2022\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received. Our company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of predicting tweets;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87ff12",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4893dd",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec36dd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/emmzytamara/advance-classification/a0af1e4929364dca80eb5700ebdb8dc2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"UOUYhOBT87Sbsbc0BSahH0VZ5\",\n",
    "    project_name=\"Advance Classification\",\n",
    "    workspace=\"emmzytamara\",\n",
    "    log_code = True\n",
    ")\n",
    "\n",
    "# Run your code and go to /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5aeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7deed07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to D:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22712/4262074385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# Setting global constants to ensure notebook results are reproducible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Libraries for importing and loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for data preparation \n",
    "import re\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Libraries for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Libraries for assessing model accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0c406",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading Data\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43727c",
   "metadata": {},
   "source": [
    "**2.1 Brief description of the data**\n",
    "\n",
    "The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "**Class Description**\n",
    "\n",
    ">2 News: the tweet links to factual news about climate change\n",
    "\n",
    ">1 Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    ">0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "\n",
    ">-1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "**Variable definitions**\n",
    "\n",
    ">sentiment: Sentiment of tweet\n",
    "\n",
    ">message: Tweet body\n",
    "\n",
    ">tweetid: Twitter unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "samplesubmission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# Preview train dataset\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fdeb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview train dataset\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview train dataset\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f17ea9",
   "metadata": {},
   "source": [
    "In this section train, test and samplesubmission data have been uploaded to have a snap shot of how the data looks like. The train data will be used to train the model and the test data will be used to test the accuracy of the model in predicting unseen data. The sample submission file will aid in uploading the model in Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021cc57",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff9981",
   "metadata": {},
   "source": [
    "### 3.1 Exploratory Data Analysis\n",
    ">>>*What is Exploratory data analysis?*\n",
    "\n",
    ">>>Exploratory data analysis (EDA) is the process of analysing and investigating data sets and summarizing their main characteristics, often employing both non-graphical and graphical methods. \n",
    "\n",
    ">>>*Why is conducting EDA important?*\n",
    "\n",
    ">>>It aids in determining how best to manipulate data to get the required answers, expose trends, patterns, and relationships that are not readily apparent i.e. get insights into the dataset.\n",
    "\n",
    ">>>*How is EDA conducted?*\n",
    "\n",
    "EDA can be conducted in the following ways:\n",
    "- **Univariate**:- \\\n",
    "    i. **non-graphical**:- This is simplest form of data analysis, where the data being analyzed consists of just one variable. Since it’s a single variable, it doesn’t deal with causes or relationships.\\\n",
    "    ii. **graphical**:- Non-graphical methods don’t provide a full picture of the data. Graphical methods are therefore required. It involves visual exploratory analysis of the data.\n",
    "- **Multivariate**:-  \\\n",
    "    i. **non-graphical**:- Multivariate non-graphical EDA techniques generally show the relationship between two or more variables of the data through cross-tabulation or statistics. \\\n",
    "    ii. **graphical**:- Multivariate data uses graphics to display relationships between two or more sets of data. The most used graphic is a grouped bar plot or bar chart with each group representing one level of one of the variables and each bar within a group representing the levels of the other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe77c9",
   "metadata": {},
   "source": [
    "### 3.2 Univariate Non-Graphical Analysis\n",
    ">For this analysis, we are going to view dataset on the below checks:  \\\n",
    "    >>i.  Check for the presence of *null* values  \\\n",
    "    >>ii. Descriptive statistical values *mean, std, minimum, quatiles, maximum, and kurtosis*  \n",
    "    >>iii. Dataset data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types for all columns\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab9928",
   "metadata": {},
   "source": [
    "The dataset has 3 features namely sentiment, message, tweetid. The features have 15819 entries and they have no missing values. The message feature has an object datetype i.e. that the features has mixed data types(numbers and strings).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36205ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a716a",
   "metadata": {},
   "source": [
    "This confirms that the dataset features have no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ce79d",
   "metadata": {},
   "source": [
    " **Descriptive Statistics**\n",
    "\n",
    ">Descriptive statistics summarize the data by computing mean, median, mode, standard deviation likewise.descriptive statistics describe the dataset in a way simpler manner through;\n",
    "\n",
    "*   The measure of central tendency \n",
    ">*  Mean:- The average value \n",
    ">*  Median:- The mid point value \n",
    ">*  Mode:- The most common value\n",
    "\n",
    "*   Measure of spread  \n",
    ">* Percentiles:- Percentiles are used in statistics to give you a number that describes the value that a given percent of the values are lower than.\n",
    ">* standard deviation:-a number that describes how spread out the values are.\n",
    "*  Measure of symmetry \n",
    ">* Skewness:- a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\n",
    ">>* If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n",
    ">>* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n",
    ">>* If skewness is between -0.5 and 0.5, the distribution is approximately symmetric. \n",
    "*  Measure of Peakedness \n",
    ">* Kurtosis:-  a measure of relative peakedness of a probability distribution, or alternatively how heavy or how light its tails are. A standard normal distribution has kurtosis of 3 and is recognized as mesokurtic. An increased kurtosis (>3) can be visualized as a thin “bell” with a high peak whereas a decreased kurtosis corresponds to a broadening of the peak and “thickening” of the tails. Kurtosis >3 is recognized as leptokurtic and <3 as platykurtic (lepto=thin; platy=broad).\n",
    ">>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de97fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "df_train.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a779488",
   "metadata": {},
   "source": [
    "From the above analysis thus far, it is evidence that we only have two numeric colunms. \n",
    "However we suspect that one of these columns(tweetid) contains unique values in each row, while the other column(sentiment) from the name, we infere that it is our label, hence contains a minimum of two different values.\n",
    "\n",
    ">To confirm the above, we write a function that takes in a dataframe and a column-id, to give an output which is the number of unique values in the column as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_val(df, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a column name, \n",
    "        and ouputs an interger, which is the number of unique \n",
    "        values in the column.\n",
    "    \"\"\"\n",
    "    return df[col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bfde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the numbers of unique values for the sentiment column\n",
    "print(f'The numbers of unique values in the sentiment column is : {unique_val(df_train, \"sentiment\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3eb2cd",
   "metadata": {},
   "source": [
    "From the codes above the sentiment feature has four unique values. Most observations are pro climate change indicated by 1. There is also an indication of class imbalance i.e. the number of observations across the classes(1,2,0,-1) is unevenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c411d",
   "metadata": {},
   "source": [
    "### 3.3 Univariate graphical inspection of data\n",
    ">For this analysis, we view the individual colunms using histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of label classes\n",
    "fig,ax = plt.subplots()\n",
    "df_train['sentiment'].value_counts().plot(kind = 'bar', facecolor='g', alpha=0.65)\n",
    "ax.set_xlabel('Sentiments')\n",
    "ax.set_ylabel('Sentiments count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbd86c",
   "metadata": {},
   "source": [
    "### 3.4 Put in Word Cloud Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for word cloud\n",
    "df_train_cloud = df_train.copy()\n",
    "df_train_cloud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04441fbd",
   "metadata": {},
   "source": [
    "The use of preprocessed data in Word Cloud makes it easy to identify the relevant words as opposed to many instances of https and other types of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproceessing functions\n",
    "def preprocess_tweet(df, col):\n",
    "    \"\"\" Functions takes in a pandas dataframe and performs a series of steps on \n",
    "        the tweet/message/text column\"\"\"\n",
    "    # Converting the whole text to lowercase\n",
    "    df[col] = df[col]. apply(lambda x: x.lower())\n",
    "\n",
    "    # Removing the twitter usernames from tweet string\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r'@\\w+', ' ', x))\n",
    "\n",
    "    # Removing the URLS from the tweet string\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r'@\\w+', ' ', x))\n",
    "\n",
    "    # Removing the URLS from the tweet string\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r'http\\S+', ' ', x))\n",
    "\n",
    "    # Deleting everything that is not characters\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r'[^a-z A-Z]', ' ', x))\n",
    "\n",
    "    # Deleting any word which is less than 3-characters mostly those are stopwords\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))\n",
    "\n",
    "    # Stripping extra spaces in the text\n",
    "    df[col] = df[col]. apply(lambda x: re.sub(r' +', ' ', x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "preprocess_tweet(df_train_cloud, 'message')\n",
    "\n",
    "df_train_cloud.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fa1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for Each Sentiment\n",
    "df_sent1 = df_train_cloud[df_train_cloud['sentiment']==1]\n",
    "df_sent0 = df_train_cloud[df_train_cloud['sentiment']==0]\n",
    "df_sentneg = df_train_cloud[df_train_cloud['sentiment']==-1]\n",
    "df_sent2 = df_train_cloud[df_train_cloud['sentiment']==2]\n",
    "\n",
    "tweet_All = \" \".join(review for review in df_train.message)\n",
    "tweet_sent0 = \" \".join(review for review in df_sent0.message)\n",
    "tweet_sent1 = \" \".join(review for review in df_sent1.message)\n",
    "tweet_sentneg = \" \".join(review for review in df_sentneg.message)\n",
    "tweet_sent2 = \" \".join(review for review in df_sent2.message)\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize  = (30,30))\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\n",
    "wordcloud_sent0 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_sent0)\n",
    "wordcloud_sent1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_sent1)\n",
    "wordcloud_sentneg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_sentneg)\n",
    "wordcloud_sent2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_sent2)\n",
    "\n",
    "# Display the generated image:\n",
    "ax[0].imshow(wordcloud_ALL, interpolation='bilinear')\n",
    "ax[0].set_title('All Tweets', fontsize=30)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(wordcloud_sent0, interpolation='bilinear')\n",
    "ax[1].set_title('Neutral Tweets',fontsize=30)\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(wordcloud_sent1, interpolation='bilinear')\n",
    "ax[2].set_title('Pro Climate Change',fontsize=30)\n",
    "ax[2].axis('off')\n",
    "ax[3].imshow(wordcloud_sentneg, interpolation='bilinear')\n",
    "ax[3].set_title('Anti Climate Change',fontsize=30)\n",
    "ax[3].axis('off')\n",
    "ax[4].imshow(wordcloud_sent2, interpolation='bilinear')\n",
    "ax[4].set_title('News Tweets',fontsize=30)\n",
    "ax[4].axis('off')\n",
    "\n",
    "#wordcloud.to_file(\"img/first_review.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4bc836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
